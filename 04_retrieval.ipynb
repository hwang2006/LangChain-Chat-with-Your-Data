{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d537a43-cb13-4d2e-af24-bfd08b1896b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b901d037-1b84-46bb-b57f-8bc6f74a74b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: lark in /scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages (1.1.8)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fe9cba-984c-49e2-94bd-06b5e50706e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 0.4.18\n",
      "Summary: Chroma.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: /scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages\n",
      "Requires: bcrypt, chroma-hnswlib, fastapi, grpcio, importlib-resources, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, overrides, posthog, pulsar-client, pydantic, pypika, PyYAML, requests, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8ab91c-ef23-4221-887c-91786365f1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87be2a12-e67a-45c5-92ef-3a27181a7603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e10e85-84c6-479f-ae1c-f5b430b19a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3eb247d-db11-40dc-b216-ff499ca83501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140006b8-d67b-40e7-91ec-0c9a6fcffa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).',\n",
       " 'A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.',\n",
       " 'A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffc487a-b343-42db-965b-48be1134a1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be40822e-ecd5-4324-ab12-565ab68d2b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326c85fe-997c-41fe-a20d-2fcbdc28e348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(smalldb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3bc2d3-a0b6-47c5-8ab6-a777c21365e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x2b4aebf3aef0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ace2bcb-c388-45f7-b141-b2170db58ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb59c081-1c49-4e9a-94ad-4c02b1eb6bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d32953-5cba-48f7-b056-ab2e4782c26b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49683fe-6d25-4c3b-bd92-836be8261205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f5f0fc-b10c-412a-87b5-7fd1c85bd45e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d633928-cc89-4df0-814c-fa83974864e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d4edd8-c690-4ec7-be9e-a722694a95b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e9e1a4-9a13-422a-9152-02df26827dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da790cc-644a-4222-a0e4-b496aceaec2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02cffc28-f1c8-4223-a730-0f27e31a4430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'algorithm then? So what’s different? How come  I was making all that noise earlier about \\nleast squa'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b6ec11-e1aa-47b6-97a7-6294fdf3fe7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning algorithms to teach a car how to  drive at reasonably high speeds off roads \\navoiding obsta'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[2].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a474c622-3459-4768-acf0-199a7360f94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffbc3442-0090-4b3b-b943-8a20c151d56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd98cf61-4ddf-4de6-8b5b-5f6777fc8474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 14, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 4, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f31f9682-8744-4393-af0e-1e76a02dd39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90bd0239-2fbc-45c3-bc64-2bea28f398b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28e5e8b-9b99-4eb9-bd18-bda14ed5a5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1922f4ed-ae58-42c6-8e30-e9723f03916c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0924fb7-1377-4dd4-ba52-243205e9b95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "535f0786-18b0-44ff-b422-171765426b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 14, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81a98600-9f78-4a75-a0c0-6637e5a1ade9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 14, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 4, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n"
     ]
    }
   ],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=5\n",
    ")\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23b994f6-7c38-4b97-84dd-94578668f8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearning-Lecture03  \n",
      "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
      "this class. So here’s what I want to do t oday, and some of the topics I do today may seem \n",
      "a little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline for \n",
      "today and the illogical flow of ideas. In the last lecture, we  talked about linear regression \n",
      "and today I want to talk about sort of an  adaptation of that called locally weighted \n",
      "regression. It’s very a popular  algorithm that’s actually one of my former mentors \n",
      "probably favorite machine learning algorithm.  \n",
      "We’ll then talk about a probabl e second interpretation of linear regression and use that to \n",
      "move onto our first classification algorithm, which is logistic regr ession; take a brief \n",
      "digression to tell you about something cal led the perceptron algorithm, which is \n",
      "something we’ll come back to, again, later this  quarter; and time allowing I hope to get to \n",
      "Newton’s method, which is an algorithm fo r fitting logistic regression models.  \n",
      "So this is recap where we’re talking about in the previous lecture, remember the notation \n",
      "I defined was that I used this X superscrip t I, Y superscript I to denote the I training \n",
      "example. And when we’re talking about linear regression or linear l east squares, we use \n",
      "this to denote the predicted value of “by my hypothesis H” on the input XI. And my\n",
      "--------\n",
      "Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \n",
      "Switch back to the chalkboard, please. I s hould say, this work was done about 15 years \n",
      "ago and autonomous driving has come a long way. So many of you will have heard of the \n",
      "DARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \n",
      "team's drive a car across a desert by itself.  \n",
      "So Alvin was, I think, absolutely amazing wo rk for its time, but autonomous driving has \n",
      "obviously come a long way since then. So what  you just saw was an example, again, of \n",
      "supervised learning, and in particular it was an  example of what they  call the regression \n",
      "problem, because the vehicle is trying to predict a continuous value variables of a \n",
      "continuous value steering directions , we call the regression problem.  \n",
      "And what I want to do today is talk about our first supervised learning algorithm, and it \n",
      "will also be to a regression task. So for the running example that I'm going to use \n",
      "throughout today's lecture, you're going to retu rn to the example of  trying to predict \n",
      "housing prices. So here's actually a data set collected by TA, Dan Ramage, on housing \n",
      "prices in Portland, Oregon.  \n",
      "So here's a dataset of a number of houses of different sizes, and here are their asking \n",
      "prices in thousands of dollars, $200,000. And so we  can take this data and plot it, square \n",
      "feet, best price, and so you make your other dataset like that. And the question is, given a\n",
      "--------\n",
      "Student: It’s the lowest it –  \n",
      "Instructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \n",
      "right? And the reason is, in logi stic regression this is diffe rent from before, right? The \n",
      "definition of this H subscript theta of XI is not the same as the definition I was using in \n",
      "the previous lecture. And in pa rticular this is no longer thet a transpose XI. This is not a \n",
      "linear function anymore. This is  a logistic function of theta transpose XI. Okay? So even \n",
      "though this looks cosmetically similar, even though this is similar on the surface, to the \n",
      "Bastrian descent rule I derive d last time for least squares regression this is actually a \n",
      "totally different learning algorithm. Okay? And it turns out that there’s actually no \n",
      "coincidence that you ended up with the same l earning rule. We’ll actually talk a bit more \n",
      "about this later when we talk about generalized linear models. But this is one of the most \n",
      "elegant generalized learning models that we’l l see later. That even though we’re using a \n",
      "different model, you actually ended up with wh at looks like the sa me learning algorithm \n",
      "and it’s actually no coincidence. Cool.  \n",
      "One last comment as part of a sort of l earning process, over here I said I take the \n",
      "derivatives and I ended up with this line . I didn’t want to make you sit through a long \n",
      "algebraic derivation, but later t oday or later this week, pleas e, do go home and look at our\n",
      "--------\n",
      "when you had a Q’s tow. Like you make it too small in your –  \n",
      "Instructor (Andrew Ng) :Yes, absolutely. Yes. So local ly weighted regression can run \n",
      "into – locally weighted regression is not a penancier for the problem  of overfitting or \n",
      "underfitting. You can still run into the same problems with locally weighted regression. \n",
      "What you just said about – and so some of these things I’ll leave you to discover for \n",
      "yourself in the homework problem. You’ll actu ally see what you just mentioned. Yeah?  \n",
      "Student: It almost seems like you’re not even th oroughly [inaudible] w ith this locally \n",
      "weighted, you had all the data th at you originally had anyway.\n",
      "--------\n",
      "really makes a difference between a good so lution and amazing solution. And to give \n",
      "everyone to just how we do points assignments, or what is it that causes a solution to get \n",
      "full marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \n",
      "way to do that.  \n",
      "Graders are paid positions and you also get free  food, and it's usually fun for us to sort of \n",
      "hang out for an evening and grade all the a ssignments. Okay, so I will send email. So \n",
      "don't email me yet if you want to be a grader. I'll send email to the entire class later with \n",
      "the administrative details and to solicit app lications. So you can email us back then, to \n",
      "apply, if you'd be interested in being a grader.  \n",
      "Okay, any questions about that? All right, okay, so let's get started with today's material. \n",
      "So welcome back to the second lecture. What  I want to do today is talk about linear \n",
      "regression, gradient descent, and the norma l equations. And I should also say, lecture \n",
      "notes have been posted online and so if some  of the math I go over today, I go over rather \n",
      "quickly, if you want to see every equation wr itten out and work through the details more \n",
      "slowly yourself, go to the course homepage and download detailed lecture notes that \n",
      "pretty much describe all the mathematical, te chnical contents I'm going to go over today.  \n",
      "Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.page_content)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b47e279f-9934-4529-86d9-518c8827d9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 7, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "MachineLearning-Lecture03  \n",
      "Instructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \n",
      "this class. So here’s what I want to do t oday, and some of the topics I do today may seem \n",
      "a little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline for \n",
      "today and the illogical flow of ideas. In the last lecture, we  talked about linear regression \n",
      "and today I want to talk about sort of an  adaptation of that called locally weighted \n",
      "regression. It’s very a popular  algorithm that’s actually one of my former mentors \n",
      "probably favorite machine learning algorithm.  \n",
      "We’ll then talk about a probabl e second interpretation of linear regression and use that to \n",
      "move onto our first classification algorithm, which is logistic regr ession; take a brief \n",
      "digression to tell you about something cal led the perceptron algorithm, which is \n",
      "something we’ll come back to, again, later this  quarter; and time allowing I hope to get to \n",
      "Newton’s method, which is an algorithm fo r fitting logistic regression models.  \n",
      "So this is recap where we’re talking about in the previous lecture, remember the notation \n",
      "I defined was that I used this X superscrip t I, Y superscript I to denote the I training \n",
      "example. And when we’re talking about linear regression or linear l east squares, we use \n",
      "this to denote the predicted value of “by my hypothesis H” on the input XI. And my\n",
      "------------\n",
      "Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \n",
      "Switch back to the chalkboard, please. I s hould say, this work was done about 15 years \n",
      "ago and autonomous driving has come a long way. So many of you will have heard of the \n",
      "DARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \n",
      "team's drive a car across a desert by itself.  \n",
      "So Alvin was, I think, absolutely amazing wo rk for its time, but autonomous driving has \n",
      "obviously come a long way since then. So what  you just saw was an example, again, of \n",
      "supervised learning, and in particular it was an  example of what they  call the regression \n",
      "problem, because the vehicle is trying to predict a continuous value variables of a \n",
      "continuous value steering directions , we call the regression problem.  \n",
      "And what I want to do today is talk about our first supervised learning algorithm, and it \n",
      "will also be to a regression task. So for the running example that I'm going to use \n",
      "throughout today's lecture, you're going to retu rn to the example of  trying to predict \n",
      "housing prices. So here's actually a data set collected by TA, Dan Ramage, on housing \n",
      "prices in Portland, Oregon.  \n",
      "So here's a dataset of a number of houses of different sizes, and here are their asking \n",
      "prices in thousands of dollars, $200,000. And so we  can take this data and plot it, square \n",
      "feet, best price, and so you make your other dataset like that. And the question is, given a\n",
      "------------\n",
      "really makes a difference between a good so lution and amazing solution. And to give \n",
      "everyone to just how we do points assignments, or what is it that causes a solution to get \n",
      "full marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \n",
      "way to do that.  \n",
      "Graders are paid positions and you also get free  food, and it's usually fun for us to sort of \n",
      "hang out for an evening and grade all the a ssignments. Okay, so I will send email. So \n",
      "don't email me yet if you want to be a grader. I'll send email to the entire class later with \n",
      "the administrative details and to solicit app lications. So you can email us back then, to \n",
      "apply, if you'd be interested in being a grader.  \n",
      "Okay, any questions about that? All right, okay, so let's get started with today's material. \n",
      "So welcome back to the second lecture. What  I want to do today is talk about linear \n",
      "regression, gradient descent, and the norma l equations. And I should also say, lecture \n",
      "notes have been posted online and so if some  of the math I go over today, I go over rather \n",
      "quickly, if you want to see every equation wr itten out and work through the details more \n",
      "slowly yourself, go to the course homepage and download detailed lecture notes that \n",
      "pretty much describe all the mathematical, te chnical contents I'm going to go over today.  \n",
      "Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so\n",
      "------------\n",
      "other is, I don’t know, I can also mumble about ju stifications, such as things to the central \n",
      "limit theorem. It turns out that if you, for th e vast majority of problems, if you apply a \n",
      "linear regression model like this and try to m easure the distribution of the errors, not all \n",
      "the time, but very often you find that the erro rs really are Gaussian. That this Gaussian \n",
      "model is a good assumption for the error in regression proble ms like these. Some of you \n",
      "may have heard of the central limit theo rem, which says that the sum of many \n",
      "independent random variables will tend towards a Gaussian. So if the error is caused by many effects, like the mood of th e seller, the mood of the buyer,  some other features that \n",
      "we miss, whether the place has a garden or not, and if all of these e ffects are independent, \n",
      "then by the central limit theorem you might be inclined to believe that the sum of all \n",
      "these effects will be approximately Gaussian. If  in practice, I guess, the two real answers \n",
      "are that, 1.) In practice this is actually a reasonably accurate assumption, and 2.) Is it \n",
      "turns out to be mathematically c onvenient to do so. Okay? Yeah?  \n",
      "Student: It seems like we’re saying if we assu me that area around model has zero mean, \n",
      "then the area is centered ar ound our model. Which it seems al most like we’re trying to \n",
      "assume what we’re trying to prove. Instructor? \n",
      "That’s the [inaudible] but, yes. You are assu ming that the error has zero mean. Which is,\n",
      "------------\n",
      "Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \n",
      "if you would like to see a refres her on linear algebra, this w eek's discussion section will \n",
      "be taught by the TAs and will be a refresher on linear algebra. So if some of the linear \n",
      "algebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \n",
      "to see some of the things I'm claiming today with our proof, if you wa nt to just see some \n",
      "of those things written out in  detail, you can come to this week's discussion section.\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(\n",
    "    question,\n",
    "    k=5\n",
    ")\n",
    "for d in docs:\n",
    "    print(d.metadata)\n",
    "\n",
    "for d in docs:\n",
    "    print(d.page_content)\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6acd54b-6c6d-4b47-b8ec-129f1cf6b758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60fe07b9-cb54-42f5-bcf9-b0e246710f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea04b921-b0fd-41d2-9932-139f1bd340f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8084aad6-c4dc-46b3-abe0-42684668f46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5f88223-f7a8-48f8-90ca-240b168d2859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"And the student said, \"Oh, it was the MATLAB.\" So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "\"And the student said, \"Oh, it was the MATLAB.\" So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "{'-' * 50}\n",
      "\n",
      "{'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n",
      "{'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)\n",
    "print(f\"\\n{'-' * 50}\\n\")\n",
    "print(\"\\n{'-' * 50}\\n\")\n",
    "for d in compressed_docs:\n",
    "    print(d.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76ceb63f-3662-4d42-a814-1b4b34cad4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n",
      "know some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn't.  \n",
      "So I guess for those of you that haven't s een MATLAB before, and I know most of you \n",
      "have, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \n",
      "plot data. And it's sort of an extremely easy to  learn tool to use for implementing a lot of \n",
      "learning algorithms.  \n",
      "And in case some of you want to work on your  own home computer or something if you \n",
      "don't have a MATLAB license, for the purposes of  this class, there's also — [inaudible] \n",
      "write that down [inaudible] MATLAB — there' s also a software package called Octave \n",
      "that you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it's free, and for the purposes of  this class, it will work for just about \n",
      "everything.  \n",
      "So actually I, well, so yeah, just a side comment for those of you that haven't seen \n",
      "MATLAB before I guess, once a colleague of mine at a different university, not at \n",
      "Stanford, actually teaches another machine l earning course. He's taught it for many years. \n",
      "So one day, he was in his office, and an old student of his from, lik e, ten years ago came \n",
      "into his office and he said, \"Oh, professo r, professor, thank you so much for your \n",
      "machine learning class. I learned so much from it. There's this stuff that I learned in your \n",
      "class, and I now use every day. And it's help ed me make lots of money, and here's a \n",
      "picture of my big house.\"  \n",
      "So my friend was very excited. He said, \"W ow. That's great. I'm glad to hear this \n",
      "machine learning stuff was actually useful. So what was it that you learned? Was it \n",
      "logistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \n",
      "learned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \n",
      "So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, \n",
      "and we'll actually have a short MATLAB tutori al in one of the discussion sections for \n",
      "those of you that don't know it.  \n",
      "Okay. The very last piece of logistical th ing is the discussion s ections. So discussion \n",
      "sections will be taught by the TAs, and atte ndance at discussion sections is optional, \n",
      "although they'll also be recorded and televi sed. And we'll use the discussion sections \n",
      "mainly for two things. For the next two or th ree weeks, we'll use the discussion sections \n",
      "to go over the prerequisites to this class or if some of you haven't seen probability or \n",
      "statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \n",
      "refresher for those of you that want one.  \n",
      "Later in this quarter, we'll also use the disc ussion sections to go over extensions for the \n",
      "material that I'm teaching in the main lectur es. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want  to teach but didn't have time in the main \n",
      "lectures for.  \n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "print(len(pages))\n",
    "print(pages[8].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2fc1439-748a-48fb-a880-24e88762ae64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"In the last lecture, we talked about linear regression\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"what you just saw was an example, again, of supervised learning, and in particular it was an example of what they call the regression problem, because the vehicle is trying to predict a continuous value variables of a continuous value steering directions, we call the regression problem.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"The definition of this H subscript theta of XI is not the same as the definition I was using in the previous lecture. And in particular this is no longer theta transpose XI. This is not a linear function anymore. This is a logistic function of theta transpose XI. Okay? So even though this looks cosmetically similar, even though this is similar on the surface, to the Bastrian descent rule I derived last time for least squares regression this is actually a totally different learning algorithm.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Instructor (Andrew Ng): \"locally weighted regression is not a penancier for the problem of overfitting or underfitting. You can still run into the same problems with locally weighted regression.\"\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 14, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 4, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "==================================================\n",
      "\"In the last lecture, we talked about linear regression\"\n",
      "==================================================\n",
      "\"what you just saw was an example, again, of supervised learning, and in particular it was an example of what they call the regression problem, because the vehicle is trying to predict a continuous value variables of a continuous value steering directions, we call the regression problem.\"\n",
      "==================================================\n",
      "\"The definition of this H subscript theta of XI is not the same as the definition I was using in the previous lecture. And in particular this is no longer theta transpose XI. This is not a linear function anymore. This is a logistic function of theta transpose XI. Okay? So even though this looks cosmetically similar, even though this is similar on the surface, to the Bastrian descent rule I derived last time for least squares regression this is actually a totally different learning algorithm.\"\n",
      "==================================================\n",
      "Instructor (Andrew Ng): \"locally weighted regression is not a penancier for the problem of overfitting or underfitting. You can still run into the same problems with locally weighted regression.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about regression in the third lecture?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "pretty_print_docs(compressed_docs)\n",
    "for d in compressed_docs:\n",
    "    print(d.metadata)\n",
    "\n",
    "for d in compressed_docs:\n",
    "    print(\"=\" * 50)\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d1ff1a7-e05d-45f2-bd80-5befc5bdd127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0d4707f-0632-4b66-9d87-a4edd4a4a194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "\"In the last lecture, we talked about linear regression\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "\"what you just saw was an example, again, of supervised learning, and in particular it was an example of what they call the regression problem, because the vehicle is trying to predict a continuous value variables of a continuous value steering directions, we call the regression problem.\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "\"If you apply a linear regression model like this and try to measure the distribution of the errors, not all the time, but very often you find that the errors really are Gaussian. That this Gaussian model is a good assumption for the error in regression problems like these. Some of you may have heard of the central limit theorem, which says that the sum of many independent random variables will tend towards a Gaussian. So if the error is caused by many effects, like the mood of the seller, the mood of the buyer, some other features that we miss, whether the place has a garden or not, and if all of these effects are independent, then by the central limit theorem you might be inclined to believe that the sum of all these effects will be approximately Gaussian. If in practice, I guess, the two real answers are that, 1.) In practice this is actually a reasonably accurate assumption, and 2.) Is it turns out to be mathematically convenient to do so. Okay? Yeah?\"\n",
      "{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n",
      "{'page': 7, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n",
      "==================================================\n",
      "\"In the last lecture, we talked about linear regression\"\n",
      "==================================================\n",
      "\"what you just saw was an example, again, of supervised learning, and in particular it was an example of what they call the regression problem, because the vehicle is trying to predict a continuous value variables of a continuous value steering directions, we call the regression problem.\"\n",
      "==================================================\n",
      "\"If you apply a linear regression model like this and try to measure the distribution of the errors, not all the time, but very often you find that the errors really are Gaussian. That this Gaussian model is a good assumption for the error in regression problems like these. Some of you may have heard of the central limit theorem, which says that the sum of many independent random variables will tend towards a Gaussian. So if the error is caused by many effects, like the mood of the seller, the mood of the buyer, some other features that we miss, whether the place has a garden or not, and if all of these effects are independent, then by the central limit theorem you might be inclined to believe that the sum of all these effects will be approximately Gaussian. If in practice, I guess, the two real answers are that, 1.) In practice this is actually a reasonably accurate assumption, and 2.) Is it turns out to be mathematically convenient to do so. Okay? Yeah?\"\n"
     ]
    }
   ],
   "source": [
    "question = \"what did they say about regression in the third lecture?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)\n",
    "for d in compressed_docs:\n",
    "    print(d.metadata)\n",
    "\n",
    "for d in compressed_docs:\n",
    "    print(\"=\" * 50)\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1a18f36-17e1-4cb5-9b22-98df78b76c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecca7d41-f0fa-4177-9385-1b7a96733c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c8edd8-cb7c-43f0-8af3-ac950044aa7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d55d1d42-b413-47f2-a216-f868b54f8aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60674"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a4895b2-32b3-45f5-bca1-33c566531515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_page_text))\n",
    "print(type(joined_page_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1cfd4d02-84f8-43a5-ab82-94e19cb07731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'bbb', 'ccc']\n",
      "aaa-bbb-ccc\n"
     ]
    }
   ],
   "source": [
    "list = [\"aaa\", \"bbb\", \"ccc\"]\n",
    "joined_list = \" \".join(list)\n",
    "print(list)\n",
    "len(\"t\".join(list))\n",
    "print(\"-\".join(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbad29af-93f8-43cb-b303-a56e2f5b58b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m342.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "482dc63e-8369-4030-a20d-7f473dc2b72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08536dbc-cbd8-40c0-8fcb-3540ad02ce47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ca2b4cd-aced-4260-bd7d-c998ebe2c4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/langchain2/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"let me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.   [End of Audio]  \\nDuration: 69 minutes\")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bc0e07c-b0a7-497f-a8f5-fe55b63d4ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Saxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look\")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5f5aff1-4466-4566-8a4e-4fbc643df79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56b91fe3-788e-4e1a-89fa-9db966a1ec9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef51333-e48d-473a-90f5-2487b02f7147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
